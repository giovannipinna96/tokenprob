# üß† LLM Token Probability Analysis System

A comprehensive toolkit for analyzing token generation probabilities in Large Language Models (LLMs), with a focus on identifying potential areas where model uncertainty might correlate with bugs or problematic code generation.

## üìñ Overview

This project implements a sophisticated analysis system for the **Qwen 2.5 Coder 7B Instruct** model that:

1. **Captures detailed probability distributions** for the top 1000 candidate tokens at each generation step
2. **Computes information theory metrics** including entropy, surprisal, and perplexity
3. **Visualizes token confidence** using color-coded representations (green = high confidence, red = low confidence)
4. **Analyzes correlations** between model uncertainty and code quality
5. **Provides interactive visualizations** and comprehensive reports

## üéØ Core Hypothesis

**Low probability tokens generated by the model may correlate with areas prone to bugs or errors in the generated code.**

This hypothesis is based on the idea that when a language model is uncertain about the next token (assigns low probability), it might indicate:
- Complex or ambiguous code patterns
- Edge cases the model hasn't seen frequently
- Potential logic errors or problematic constructs

## üèóÔ∏è Architecture

### Core Components

#### 1. `LLM.py` - Model Analysis Engine
- **QwenProbabilityAnalyzer**: Main class for model loading and analysis
- **TokenAnalysis**: Data structure storing detailed metrics for each token
- Implements custom generation loop to capture:
  - Raw logits before softmax
  - Top 1000 token probabilities
  - Information theory metrics
  - Token ranking and selection process

#### 2. `visualizer.py` - Visualization System
- **TokenVisualizer**: Multi-mode visualization engine
- **TokenVisualizationMode**: Different coloring schemes for various metrics
- Supports:
  - HTML visualizations with interactive tooltips
  - Matplotlib static plots
  - Plotly interactive dashboards
  - Comprehensive analysis reports

#### 3. `use_case.py` - Practical Application
- **CodeGenerationUseCase**: Binary search implementation challenge
- Includes:
  - Comprehensive test suite (13 functional + 4 error tests)
  - Reference correct implementation
  - Automated code testing and validation
  - Correlation analysis between confidence and correctness

#### 4. `example_usage.py` - Demonstration Scripts
- Multiple example scenarios
- Interactive demo mode
- Temperature comparison analysis
- Browser integration for results

## üìä Information Theory Metrics

### Probability
- **Definition**: `P(token) = softmax(logit)[token_id]`
- **Range**: [0, 1]
- **Interpretation**: Higher values indicate more confident predictions

### Entropy
- **Definition**: `H = -Œ£ P(i) * log‚ÇÇ(P(i))`
- **Range**: [0, log‚ÇÇ(vocab_size)]
- **Interpretation**: Measures uncertainty in the probability distribution
  - Low entropy: Model is confident (distribution is peaked)
  - High entropy: Model is uncertain (distribution is uniform)

### Surprisal
- **Definition**: `S = -log‚ÇÇ(P(selected_token))`
- **Range**: [0, ‚àû]
- **Interpretation**: How "surprising" the selected token was
  - Low surprisal: Expected token
  - High surprisal: Unexpected token choice

### Perplexity
- **Definition**: `PP = 2^H` (2 raised to the power of entropy)
- **Range**: [1, vocab_size]
- **Interpretation**: Effective number of choices the model is considering
  - Low perplexity: Few alternatives considered
  - High perplexity: Many alternatives considered

### Token Rank
- **Definition**: Position of selected token when all tokens are sorted by probability
- **Range**: [1, vocab_size]
- **Interpretation**: Lower ranks indicate more likely choices

## üé® Visualization Modes

### 1. Probability Mode (Default)
- **Color Scheme**: Red (low) ‚Üí Yellow ‚Üí Green (high)
- **Use Case**: Identify low-confidence tokens
- **Threshold**: Typically bottom 20th percentile flagged as concerning

### 2. Rank Mode
- **Color Scheme**: Red (high rank) ‚Üí Green (low rank)
- **Use Case**: See when model picks less obvious choices
- **Note**: Logarithmic scaling for better visualization

### 3. Entropy Mode
- **Color Scheme**: Red (high uncertainty) ‚Üí Blue (low uncertainty)
- **Use Case**: Identify decision points where model sees many alternatives

### 4. Surprisal Mode
- **Color Scheme**: Red (surprising) ‚Üí Blue (expected)
- **Use Case**: Highlight unexpected token choices

### 5. Logits Mode
- **Color Scheme**: Red (low) ‚Üí Green (high)
- **Use Case**: Raw model confidence before normalization

### 6. Perplexity Mode
- **Color Scheme**: Red (high) ‚Üí Blue (low)
- **Use Case**: Effective vocabulary size at each step

## üî¨ Use Case: Binary Search Implementation

### The Challenge
Generate a Python function implementing binary search with:
- Proper algorithm implementation
- Edge case handling
- Error validation
- Clean, readable code

### Test Suite
**Functional Tests (13):**
- Target in middle, beginning, end of array
- Target not found (larger/smaller than range)
- Single element arrays
- Empty arrays
- Arrays with duplicates
- Negative numbers
- Float numbers

**Error Tests (4):**
- None input (should raise TypeError)
- Non-list input (should raise TypeError)
- Unsorted array (should raise ValueError)
- Reverse sorted array (should raise ValueError)

### Analysis Metrics
- **Pass Rate**: Percentage of tests passed
- **Confidence Score**: Average token probability
- **Low Confidence Regions**: Areas with bottom 20th percentile probabilities
- **Correlation Hypothesis**: Relationship between confidence and correctness

## üöÄ Quick Start

### Installation
```bash
# Clone or download the project
cd tokenprob

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage
```python
from LLM import QwenProbabilityAnalyzer
from visualizer import TokenVisualizer, TokenVisualizationMode

# Initialize analyzer
analyzer = QwenProbabilityAnalyzer()

# Generate with analysis
prompt = "Write a Python function to calculate factorial"
text, analyses = analyzer.generate_with_analysis(prompt, max_new_tokens=100)

# Visualize results
visualizer = TokenVisualizer()
html_viz = visualizer.create_html_visualization(
    analyses, 
    mode=TokenVisualizationMode.PROBABILITY
)

# Save visualization
with open("result.html", "w") as f:
    f.write(html_viz)
```

### Run Examples
```bash
# Run all examples
python example_usage.py

# Run specific example
python example_usage.py --example advanced

# Run with automatic browser opening
python example_usage.py --example simple --open-browser

# Interactive demo
python example_usage.py --example interactive
```

### Run Use Case
```bash
# Full binary search analysis
python use_case.py

# This will generate:
# - binary_search_analysis.html (comprehensive report)
# - generation_analysis.json (raw data)
```

## üìà Interpreting Results

### Color Coding
- üü¢ **Green tokens**: High confidence, likely correct
- üü° **Yellow tokens**: Moderate confidence
- üî¥ **Red tokens**: Low confidence, potential problem areas

### Key Indicators

#### High Risk Signals
- **Low probability tokens** (< 0.3): Model uncertainty
- **High rank tokens** (> 100): Unusual choices
- **High entropy regions**: Many alternatives considered
- **Multiple consecutive low-confidence tokens**: Potential error zones

#### Good Signals
- **High probability tokens** (> 0.7): Confident choices
- **Low rank tokens** (< 10): Obvious, expected choices
- **Low entropy**: Clear decision making
- **Consistent confidence patterns**: Stable generation

### Correlation Patterns

#### Strong Correlation (Hypothesis Confirmed)
- Low average probability (< 0.4) + Low pass rate (< 70%)
- Many low-confidence regions + Multiple test failures
- High entropy + Functional errors

#### Positive Correlation (Hypothesis Supported)
- High average probability (> 0.6) + High pass rate (> 80%)
- Few low-confidence regions + Most tests pass
- Low entropy + Clean, correct code

## üîß Advanced Configuration

### Model Parameters
```python
analyzer = QwenProbabilityAnalyzer(
    model_name="Qwen/Qwen2.5-Coder-7B-Instruct",
    device="auto"  # or "cuda", "cpu"
)
```

### Generation Parameters
```python
text, analyses = analyzer.generate_with_analysis(
    prompt="Your prompt here",
    max_new_tokens=200,        # Longer for complex tasks
    temperature=0.1,           # Lower for more deterministic code
    top_p=0.9,                # Nucleus sampling threshold
    do_sample=True            # Enable sampling vs greedy
)
```

### Visualization Parameters
```python
# Customize confidence threshold
low_conf_regions = visualizer.identify_low_confidence_regions(
    analyses, 
    threshold_percentile=15    # Bottom 15% instead of 20%
)

# Different visualization modes
modes = [
    TokenVisualizationMode.PROBABILITY,
    TokenVisualizationMode.ENTROPY,
    TokenVisualizationMode.SURPRISAL
]
```

## üìä Example Results

### Typical Patterns

#### High-Quality Code Generation
```
Average Probability: 0.73
Pass Rate: 92%
Low Confidence Regions: 1
Pattern: Consistent green tokens with occasional yellow
```

#### Problematic Code Generation
```
Average Probability: 0.34
Pass Rate: 54%
Low Confidence Regions: 4
Pattern: Many red tokens, especially around logic errors
```

### Common Low-Confidence Areas
1. **Complex conditional logic**: `if-elif-else` chains
2. **Loop boundaries**: Off-by-one errors in ranges
3. **Edge case handling**: Empty inputs, null checks
4. **Variable naming**: Less common identifier choices
5. **Error handling**: Exception types and messages

## üß™ Research Applications

### Potential Research Directions

1. **Bug Prediction**: Train classifiers using confidence patterns
2. **Code Quality Metrics**: Develop confidence-based quality scores
3. **Model Improvement**: Use uncertainty for fine-tuning data selection
4. **Human-AI Collaboration**: Flag uncertain regions for human review
5. **Automated Testing**: Generate tests for low-confidence code sections

### Data Collection
The system generates rich datasets including:
- Token-level probability distributions
- Information theory metrics time series
- Code quality correlations
- Temperature sensitivity analysis

## ‚ö†Ô∏è Limitations and Considerations

### Model Limitations
- Analysis is model-specific (Qwen 2.5 Coder 7B Instruct)
- Requires significant computational resources
- Generation speed is slower due to analysis overhead

### Interpretation Caveats
- Correlation ‚â† Causation
- Low probability doesn't always mean errors
- High probability doesn't guarantee correctness
- Context and task complexity matter

### Performance Considerations
- Memory usage scales with sequence length
- GPU recommended for reasonable performance
- Analysis overhead ~2-3x slower than normal generation

## üõ†Ô∏è Technical Requirements

### Minimum Requirements
- Python 3.8+
- 8GB RAM (16GB recommended)
- 10GB free disk space

### Recommended Setup
- Python 3.10+
- 16GB+ RAM
- GPU with 8GB+ VRAM (RTX 3070/4060 or better)
- 20GB+ free disk space

### Dependencies
See `requirements.txt` for complete list. Key dependencies:
- `torch >= 2.0.0`
- `transformers >= 4.35.0`
- `matplotlib >= 3.5.0`
- `plotly >= 5.15.0`
- `numpy >= 1.21.0`

## üìö File Structure

```
tokenprob/
‚îú‚îÄ‚îÄ LLM.py                    # Core model analysis engine
‚îú‚îÄ‚îÄ visualizer.py             # Visualization system
‚îú‚îÄ‚îÄ use_case.py              # Binary search use case
‚îú‚îÄ‚îÄ example_usage.py         # Example scripts
‚îú‚îÄ‚îÄ requirements.txt         # Dependencies
‚îú‚îÄ‚îÄ README.md               # This documentation
‚îú‚îÄ‚îÄ main.py                 # Basic example (generated by original LLM.py)
‚îî‚îÄ‚îÄ CLAUDE.md              # Claude Code configuration
```

## ü§ù Contributing

This project demonstrates a novel approach to LLM analysis. Potential contributions:

1. **Additional Use Cases**: Implement other coding challenges
2. **New Metrics**: Develop additional information theory measures
3. **Visualization Improvements**: Enhanced interactive displays
4. **Performance Optimization**: Faster analysis algorithms
5. **Model Support**: Extend to other LLMs (GPT, Claude, etc.)

## üìÑ License

This project is for research and educational purposes. Please ensure compliance with model licenses and usage terms.

## üéØ Future Work

### Immediate Enhancements
- [ ] Batch processing for multiple prompts
- [ ] Real-time streaming analysis
- [ ] Integration with popular IDEs
- [ ] A/B testing framework for different models

### Research Extensions
- [ ] Multi-model comparison analysis
- [ ] Longitudinal studies on code quality prediction
- [ ] Integration with static analysis tools
- [ ] Human evaluation studies

### Technical Improvements
- [ ] Quantization support for smaller models
- [ ] Distributed computing for large-scale analysis
- [ ] API service deployment
- [ ] Web-based interactive interface

---

**üß† Understanding LLM confidence patterns is key to building better AI-assisted development tools. This system provides the foundation for exploring the relationship between model uncertainty and code quality.**